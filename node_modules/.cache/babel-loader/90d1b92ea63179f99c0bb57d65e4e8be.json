{"ast":null,"code":"'use strict';\n\nvar BB = require('bluebird');\nvar contentPath = require('./content/path');\nvar finished = BB.promisify(require('mississippi').finished);\nvar fixOwner = require('./util/fix-owner');\nvar fs = require('graceful-fs');\nvar glob = BB.promisify(require('glob'));\nvar index = require('./entry-index');\nvar path = require('path');\nvar rimraf = BB.promisify(require('rimraf'));\nvar ssri = require('ssri');\nBB.promisifyAll(fs);\nmodule.exports = verify;\nfunction verify(cache, opts) {\n  opts = opts || {};\n  opts.log && opts.log.silly('verify', 'verifying cache at', cache);\n  return BB.reduce([markStartTime, fixPerms, garbageCollect, rebuildIndex, cleanTmp, writeVerifile, markEndTime], function (stats, step, i) {\n    var label = step.name || \"step #\".concat(i);\n    var start = new Date();\n    return BB.resolve(step(cache, opts)).then(function (s) {\n      s && Object.keys(s).forEach(function (k) {\n        stats[k] = s[k];\n      });\n      var end = new Date();\n      if (!stats.runTime) {\n        stats.runTime = {};\n      }\n      stats.runTime[label] = end - start;\n      return stats;\n    });\n  }, {}).tap(function (stats) {\n    stats.runTime.total = stats.endTime - stats.startTime;\n    opts.log && opts.log.silly('verify', 'verification finished for', cache, 'in', \"\".concat(stats.runTime.total, \"ms\"));\n  });\n}\nfunction markStartTime(cache, opts) {\n  return {\n    startTime: new Date()\n  };\n}\nfunction markEndTime(cache, opts) {\n  return {\n    endTime: new Date()\n  };\n}\nfunction fixPerms(cache, opts) {\n  opts.log && opts.log.silly('verify', 'fixing cache permissions');\n  return fixOwner.mkdirfix(cache, opts.uid, opts.gid).then(function () {\n    // TODO - fix file permissions too\n    return fixOwner.chownr(cache, opts.uid, opts.gid);\n  }).then(function () {\n    return null;\n  });\n}\n\n// Implements a naive mark-and-sweep tracing garbage collector.\n//\n// The algorithm is basically as follows:\n// 1. Read (and filter) all index entries (\"pointers\")\n// 2. Mark each integrity value as \"live\"\n// 3. Read entire filesystem tree in `content-vX/` dir\n// 4. If content is live, verify its checksum and delete it if it fails\n// 5. If content is not marked as live, rimraf it.\n//\nfunction garbageCollect(cache, opts) {\n  opts.log && opts.log.silly('verify', 'garbage collecting content');\n  var indexStream = index.lsStream(cache);\n  var liveContent = new Set();\n  indexStream.on('data', function (entry) {\n    if (opts && opts.filter && !opts.filter(entry)) {\n      return;\n    }\n    liveContent.add(entry.integrity.toString());\n  });\n  return finished(indexStream).then(function () {\n    var contentDir = contentPath._contentDir(cache);\n    return glob(path.join(contentDir, '**'), {\n      follow: false,\n      nodir: true,\n      nosort: true\n    }).then(function (files) {\n      return BB.resolve({\n        verifiedContent: 0,\n        reclaimedCount: 0,\n        reclaimedSize: 0,\n        badContentCount: 0,\n        keptSize: 0\n      }).tap(function (stats) {\n        return BB.map(files, function (f) {\n          var split = f.split(/[/\\\\]/);\n          var digest = split.slice(split.length - 3).join('');\n          var algo = split[split.length - 4];\n          var integrity = ssri.fromHex(digest, algo);\n          if (liveContent.has(integrity.toString())) {\n            return verifyContent(f, integrity).then(function (info) {\n              if (!info.valid) {\n                stats.reclaimedCount++;\n                stats.badContentCount++;\n                stats.reclaimedSize += info.size;\n              } else {\n                stats.verifiedContent++;\n                stats.keptSize += info.size;\n              }\n              return stats;\n            });\n          } else {\n            // No entries refer to this content. We can delete.\n            stats.reclaimedCount++;\n            return fs.statAsync(f).then(function (s) {\n              return rimraf(f).then(function () {\n                stats.reclaimedSize += s.size;\n                return stats;\n              });\n            });\n          }\n        }, {\n          concurrency: opts.concurrency || 20\n        });\n      });\n    });\n  });\n}\nfunction verifyContent(filepath, sri) {\n  return fs.statAsync(filepath).then(function (stat) {\n    var contentInfo = {\n      size: stat.size,\n      valid: true\n    };\n    return ssri.checkStream(fs.createReadStream(filepath), sri).catch(function (err) {\n      if (err.code !== 'EINTEGRITY') {\n        throw err;\n      }\n      return rimraf(filepath).then(function () {\n        contentInfo.valid = false;\n      });\n    }).then(function () {\n      return contentInfo;\n    });\n  }).catch({\n    code: 'ENOENT'\n  }, function () {\n    return {\n      size: 0,\n      valid: false\n    };\n  });\n}\nfunction rebuildIndex(cache, opts) {\n  opts.log && opts.log.silly('verify', 'rebuilding index');\n  return index.ls(cache).then(function (entries) {\n    var stats = {\n      missingContent: 0,\n      rejectedEntries: 0,\n      totalEntries: 0\n    };\n    var buckets = {};\n    for (var k in entries) {\n      if (entries.hasOwnProperty(k)) {\n        var hashed = index._hashKey(k);\n        var entry = entries[k];\n        var excluded = opts && opts.filter && !opts.filter(entry);\n        excluded && stats.rejectedEntries++;\n        if (buckets[hashed] && !excluded) {\n          buckets[hashed].push(entry);\n        } else if (buckets[hashed] && excluded) {\n          // skip\n        } else if (excluded) {\n          buckets[hashed] = [];\n          buckets[hashed]._path = index._bucketPath(cache, k);\n        } else {\n          buckets[hashed] = [entry];\n          buckets[hashed]._path = index._bucketPath(cache, k);\n        }\n      }\n    }\n    return BB.map(Object.keys(buckets), function (key) {\n      return rebuildBucket(cache, buckets[key], stats, opts);\n    }, {\n      concurrency: opts.concurrency || 20\n    }).then(function () {\n      return stats;\n    });\n  });\n}\nfunction rebuildBucket(cache, bucket, stats, opts) {\n  return fs.truncateAsync(bucket._path).then(function () {\n    // This needs to be serialized because cacache explicitly\n    // lets very racy bucket conflicts clobber each other.\n    return BB.mapSeries(bucket, function (entry) {\n      var content = contentPath(cache, entry.integrity);\n      return fs.statAsync(content).then(function () {\n        return index.insert(cache, entry.key, entry.integrity, {\n          uid: opts.uid,\n          gid: opts.gid,\n          metadata: entry.metadata\n        }).then(function () {\n          stats.totalEntries++;\n        });\n      }).catch({\n        code: 'ENOENT'\n      }, function () {\n        stats.rejectedEntries++;\n        stats.missingContent++;\n      });\n    });\n  });\n}\nfunction cleanTmp(cache, opts) {\n  opts.log && opts.log.silly('verify', 'cleaning tmp directory');\n  return rimraf(path.join(cache, 'tmp'));\n}\nfunction writeVerifile(cache, opts) {\n  var verifile = path.join(cache, '_lastverified');\n  opts.log && opts.log.silly('verify', 'writing verifile to ' + verifile);\n  return fs.writeFileAsync(verifile, '' + +new Date());\n}\nmodule.exports.lastRun = lastRun;\nfunction lastRun(cache) {\n  return fs.readFileAsync(path.join(cache, '_lastverified'), 'utf8').then(function (data) {\n    return new Date(+data);\n  });\n}","map":null,"metadata":{},"sourceType":"script"}